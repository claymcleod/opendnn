PAPERS.txt

:Author: Clay L. McLeod
:Email: clay.l.mcleod@gmail.com
:Date: 2016-01-23 17:38

###############
# Activations #
###############

[A1] Sigmoid and Tanh layers discussed in-depth => "Efficient Backprop" (http://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf)
[A2] Softsign introduction => "Quadratic Polynomials Learn Better Image Features" (http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205)
[A3] Rectified Linear Unit (ReLU) introduction => "Rectified Linear Units Improve Restricted Boltzmann Machines" (http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)
[A4] Leaky ReLU (LReLU) introduction => "Rectifier Nonlinearities Improve Neural Network Acoustic Models" (https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)
[A5] Parametric ReLU (PReLU) introduction => "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" (http://arxiv.org/abs/1502.01852)

###################
# Initializations #
###################


[I1] Lecun's Uniform Distribution => "Efficient Backprop" (http://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf)
[I2] Glorot initializations => "Understanding the difficulty of training deep feedforward neural networks" (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
[I3] Saxe orthogonal initializations => "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks" (http://arxiv.org/abs/1312.6120)
[I4] He initializations => "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" (http://arxiv.org/abs/1502.01852)

##########
# Layers #
##########

[L1] Long Short Term Memory (LSTM) layer introduction => "Long Short Term Memory" (http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)


#######################
# Training Techniques #
#######################

[T1] Convolutional training stages => "Very Deep Convolutional Networks for Large-Scale Image Recognition" (http://arxiv.org/abs/1409.1556)
[T2] Thin deep net training stages => "FitNets: Hints for Thin Deep Nets" (http://arxiv.org/abs/1412.6550)
[T3] Loss functions at different training stages => "Going Deeper with Convolutions" (http://arxiv.org/abs/1409.4842)
[T4] Deeply supervised nets training stages => "Deeply-Supervised Nets" (http://jmlr.org/proceedings/papers/v38/lee15a.pdf)


########################
# Other notable papers #
########################

[*] ReLUs in speech => "On Rectified Linear Units For Speech Processing" (http://research.google.com/pubs/archive/40811.pdf)
[*] Local Winner Take All (LWTA) blocks => "Compete to Compute" (http://papers.nips.cc/paper/5059-compete-to-compute.pdf)
