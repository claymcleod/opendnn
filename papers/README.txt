PAPERS.txt

:Author: Clay L. McLeod
:Email: clay.l.mcleod@gmail.com
:Date: 2016-01-23 17:38

###############
# Activations #
###############

[A1] Softsign introduction => (http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205)
[A2] Rectified Linear Unit (ReLU) introduction => (http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)
[A3] Leaky ReLU (LReLU) introduction => (https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)
[A4] Parametric ReLU (PReLU) introduction => (http://arxiv.org/abs/1502.01852)

###################
# Initializations #
###################

[I1] Glorot initializations => (http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
[I2] He initializations => (http://arxiv.org/abs/1502.01852)
[I3] Saxe orthogonal initializations => (http://arxiv.org/abs/1312.6120)
[I4] Lecun's Uniform Distribution => (http://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf)


#######################
# Training Techniques #
#######################

[T1] Convolutional training stages => (http://arxiv.org/abs/1409.1556)
[T2] Thin deep net training stages => (http://arxiv.org/abs/1412.6550)
[T3] Loss functions at different training stages => (http://arxiv.org/abs/1409.4842)
[T4] Deeply supervised nets training stages => (http://jmlr.org/proceedings/papers/v38/lee15a.html)


########################
# Other notable papers #
########################

[*] ReLUs in speech => (http://research.google.com/pubs/archive/40811.pdf)
[*] Local Winner Take All (LWTA) blocks => (http://papers.nips.cc/paper/5059-compete-to-compute.pdf)
