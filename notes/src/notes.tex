\documentclass[12pt]{article}

\usepackage{amsmath}

\title{Notes on Deep Neural Networks}
\author{Clay L. McLeod --- \texttt{clay.l.mcleod@gmail.com}}

\begin{document}
\maketitle

\section{Neural Network Basics}
\subsection{Weight Initializations}

\subsubsection{Lecun's Distribution}

Presented in \cite[Sec 4.6]{efficient-backprop}, Lecun's distribution assumes a linear model and is based on the following argument: suppose we have a neural network activation layer that uses $tanh(X)$ as a nonlinear squashing function. In order for convergence to occur quickly, the weights should be initialized so that (1) the weights are not too small, causing the gradient function to be small and (2) the $tanh$ is not saturated (the weights are not too large), also causing
the gradient function to be small. Assuming that the data is properly normalized, all we need to do is initialize the weights to have $\mu=0$ and $\sigma=1$. Thus, the equations for Lecun's distribution are as follows:

\begin{equation}
    X \sim N \left[~-\frac{1}{\sqrt{n_{j}}},~\frac{1}{\sqrt{n_{j}}}~\right]\\
    \label{eq:lecun:normal}
\end{equation}


\begin{equation}
    X \sim U \left[~-\frac{3}{\sqrt{n_{j}}},~\frac{3}{\sqrt{n_{j}}}~\right]
    \label{eq:lecun:uniform}
\end{equation}

\subsubsection{Glorot Distribution}

Presented in \cite[4.2]{glorot10}.

\begin{equation}
    X \sim N \left[~-\frac{\sqrt{2}}{\sqrt{n_{j} + n_{j+1}}},~\frac{\sqrt{2}}{\sqrt{n_{j} + n_{j+1}}}~\right]\\
    \label{eq:glorot:normal}
\end{equation}

\begin{equation}
    X \sim U \left[~-\frac{\sqrt{6}}{\sqrt{n_{j} + n_{j+1}}},~\frac{\sqrt{6}}{\sqrt{n_{j} + n_{j+1}}}~\right]
    \label{eq:glorot:uniform}
\end{equation}

\subsubsection{He Distribution}

Previous assumes linear activation function, which is not suitable for ReLU or its derivatives. Thus, the authors of \cite{he15}[pg. 4] derive a theoretically sound initialization for networks utilizing the ReLU activation family.

\begin{equation}
    X \sim N \left[~-\sqrt{\frac{2}{n_{j}}},~\sqrt{\frac{2}{n_{j}}}~\right]\\
    \label{eq:he:normal}
\end{equation}

\begin{equation}
    X \sim U \left[~-\sqrt{\frac{6}{n_{j}}},~\sqrt{\frac{6}{n_{j}}}~\right]
    \label{eq:he:uniform}
\end{equation}

\bibliographystyle{ieeetr}
\bibliography{notes}
\end{document}
